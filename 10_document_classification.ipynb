{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Ud7V89xZ504GreKsw6yTFAkk9U45S3FR",
      "authorship_tag": "ABX9TyOSTu3e5jaSunQrXhDsRQC8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-sml/blob/main/10_document_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77taeHZLUaTm"
      },
      "source": [
        "# 機械学習で文書分類を試みる\n",
        "\n",
        "* WikipediaからUSの男性俳優と女性俳優のページを2020年にクローリングして、spaCyで簡単な前処理を済ませてあるデータを使う。\n",
        "\n",
        " * 固有名詞、冠詞、前置詞、代名詞、副詞、数詞、接続詞は除去してある。\n",
        " * lemmatizationした結果を使っている。\n",
        " * \"actor\"と\"actress\"という単語は特別に除去してある。\n",
        "\n",
        "* 分析の目的1: 検証データでできるかぎりチューニングを行い、最後にテストデータでの分類性能を明らかにする。\n",
        "\n",
        "* 分析の目的2: 男性俳優と女性俳優のページを分類する際に、どのような単語が特に効いているかを調べる。\n",
        "\n",
        " * この調査によって、俳優に関する記述におけるジェンダー・ステレオタイプを明らかにできるか？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qos6P0RsSsqg"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import roc_auc_score, auc, roc_curve\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEMUit1zVZma"
      },
      "source": [
        "## 1) データファイルを読み込む\n",
        "\n",
        "* データファイルは、あらかじめ自分のGoogle Driveの適当なフォルダに置いておく。\n",
        "\n",
        "* データファイルの各行には、女性俳優(1)か男性俳優(0)かを表すフラグ、俳優の名前、Wikipediaのページの本文が、この順に格納されている。\n",
        "\n",
        "* データファイルの各行に対してeval組み込み関数を適用すると、Pythonのリストに変換できるようなフォーマットで、ファイルに記録されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngsjTPa8TqgP"
      },
      "source": [
        "y = list()\n",
        "names = list()\n",
        "corpus = list()\n",
        "with open('/content/drive/MyDrive/data/us_actors_and_actresses.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    flag, name, text = eval(line.strip())\n",
        "    y.append(int(flag))\n",
        "    names.append(name)\n",
        "    corpus.append(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pz_yuM4CEYt"
      },
      "source": [
        "* `corpus`は、本文の文字列がたくさん入っているリスト。最初の10件の文書だけ表示してみる。\n",
        " * ちなみにbearはbornの原型なので、多く含まれる。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48lbVvkseCqk"
      },
      "source": [
        "corpus[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0epIPR0CH8f"
      },
      "source": [
        "* `y`は、各文書が男性俳優に関するものか、女性俳優に関するものかを表す、0/1の値（これが目的変数）。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(y)\n",
        "print(y.sum(), len(y))"
      ],
      "metadata": {
        "id": "eNg4Ysu3JjJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `names`は、各文書が誰に関する記事かを表す俳優の名前（分類には使わない参考データ）。"
      ],
      "metadata": {
        "id": "NDrt5PFJJnY3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB1KVAZ6YB3E"
      },
      "source": [
        "names = np.array(names)\n",
        "print(names[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR0Ew63dY-1I"
      },
      "source": [
        "N = len(y) # 全文書数\n",
        "print(f'We have {N:d} documents.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvlpBs-hGzML"
      },
      "source": [
        "## 2) テストデータを分離"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names_train, names_test, y_train, y_test = train_test_split(list(zip(names, corpus)), y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "pqoOCP6qJ-te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names_train, corpus_train = zip(*names_train)\n",
        "names_test, corpus_test = zip(*names_test)"
      ],
      "metadata": {
        "id": "9HRmTtPoKtRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(names_train), len(names_test))"
      ],
      "metadata": {
        "id": "2u2cXXVuLFrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZFZGWwte8fu"
      },
      "source": [
        "**training setとtest setへの分割は、変えないようにしてください。**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKg-InjbVh7r"
      },
      "source": [
        "## 3) TF-IDFで各文書をベクトル化する\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQWEdxwuC7Tt"
      },
      "source": [
        "### TF-IDFとは\n",
        "* TF-IDFは単語列をベクトル化する方法のひとつ。\n",
        "* ベクトルの次元は語彙数となる。各文書が1つのベクトルへ変換される。\n",
        "\n",
        " * すべての単語について、文書ごとに重みを計算する方法。\n",
        " * TFとは、ある文書のなかにその単語が何回出現するか、その回数。\n",
        " * DFとは、単語がいくつの文書に出現するか、その文書数。IDFはDFの逆数。\n",
        " * TF-IDFは、ざっくり言うと、TFとIDFの積。\n",
        " * 特定の文書に注目すると、その文書に出現する回数が多いほど、TF-IDFの値は大きくなる。しかし、たくさんの文書に出現する単語は、IDFが小さくなるので、その分、TF-IDFの値は小さくなる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbLfK3LvDEMf"
      },
      "source": [
        "### sklearnの`TfidfVectorizer`について\n",
        "* TfidfVectorizerのパラメータをチューニングしても構わないです。\n",
        "\n",
        "* ここでTF-IDFの計算をするとき、テストデータは使っていないので、ズルはしていない。\n",
        "* stop_words='english'とは、英語のストップワードは語彙から取り除く、という意味。\n",
        "  * ストップワードとはthe, a, is, ofなど、ありふれすぎていて、文書分類などのタスクにはあまり効かない単語のこと。こういう単語は、特徴量削減の意味も含めて、しばしばあらかじめ削除しておく。\n",
        "\n",
        "* min_dfは、その数より少ない文書にしか出現しない単語を削除する、という意味のパラメータ。希少な単語を削除するために使う。\n",
        "\n",
        "* max_dfは、0から1の間の実数で指定すると、その割合より多い文書に出現する単語を削除する、という意味のパラメータ。ありふれた単語を削除するために使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHsek-uzTxi0"
      },
      "source": [
        "# TF-IDFにより、単なる単語列であった文書を、ベクトル化する。\n",
        "# （ベクトル化してしまえば、テキストデータのような非構造化データも、\n",
        "#  様々な機械学習手法の入力として使えるようになる。）\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', min_df=50, smooth_idf=False, sublinear_tf=True)\n",
        "X_train = vectorizer.fit_transform(corpus_train)\n",
        "print('X_train: 文書数　{}, 語彙数 {}'.format(*X_train.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy5RKu8zDZVP"
      },
      "source": [
        "* 語彙を取得する。\n",
        " * NumPyの配列に変換しておく。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUcTnazzT5Qm"
      },
      "source": [
        "vocab = np.array(vectorizer.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70HQNALpDcQl"
      },
      "source": [
        "* 語彙の一部を見てみる（アルファベット順に並んでいるようだ）。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e2QrURgT_gr"
      },
      "source": [
        "print(vocab[1000:1010])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YApqwsSrDeJK"
      },
      "source": [
        "* 最初の文書をTF-IDFでベクトル化したらどうなったかを、見てみる。\n",
        " * スパースな表現になっている。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GZx-gLj2QRf"
      },
      "source": [
        "print(type(X_train))\n",
        "print(X_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmRAzGE1W4af"
      },
      "source": [
        "## 4) ロジスティック回帰による２値分類と評価"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT5NR_zjE3Z6"
      },
      "source": [
        "* 交差検証のための準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsjXWq7-EX2O"
      },
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlKIK8jlT_SO"
      },
      "source": [
        "### 正則化のハイパーパラメータ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMBWsc8wS_9o"
      },
      "source": [
        "for C in 10. ** np.arange(-2, 3):\n",
        "  scores = list()\n",
        "  for train_index, valid_index in skf.split(X_train, y_train):\n",
        "    clf = LogisticRegression(C=C, solver='liblinear', random_state=123)\n",
        "    clf.fit(X_train[train_index], y_train[train_index])\n",
        "    score = clf.score(X_train[valid_index], y_train[valid_index])\n",
        "    print(f'\\tscore {score:.4f}')\n",
        "    scores.append(score)\n",
        "  print(f'mean validation accuracy: {np.array(scores).mean():.4f} for C={C:.2e}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOOvEgPQVqVI"
      },
      "source": [
        "* L1正則化も試す。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_K4tVR6Vr5E"
      },
      "source": [
        "for C in 10. ** np.arange(-2, 3):\n",
        "  scores = list()\n",
        "  for train_index, valid_index in skf.split(X_train, y_train):\n",
        "    clf = LogisticRegression(penalty='l1', C=C, solver='liblinear', random_state=123)\n",
        "    clf.fit(X_train[train_index], y_train[train_index])\n",
        "    score = clf.score(X_train[valid_index], y_train[valid_index])\n",
        "    print(f'\\tscore {score:.4f}')\n",
        "    scores.append(score)\n",
        "  print(f'mean validation accuracy: {np.array(scores).mean():.4f} for C={C:.2e}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "190x80QqULfi"
      },
      "source": [
        "### TF-IDFの設定\n",
        "* まずmin_dfを変えてみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvSnZVQ-T-Lx"
      },
      "source": [
        "for min_df in [10, 20, 50, 100]:\n",
        "\n",
        "  vectorizer = TfidfVectorizer(stop_words='english', min_df=min_df, smooth_idf=False, sublinear_tf=True)\n",
        "  X_train = vectorizer.fit_transform(corpus_train)\n",
        "\n",
        "  scores = list()\n",
        "  for train_index, valid_index in skf.split(X_train, y_train):\n",
        "    clf = LogisticRegression(C=1.0, solver='liblinear', random_state=123)\n",
        "    clf.fit(X_train[train_index], y_train[train_index])\n",
        "    scores.append(clf.score(X_train[valid_index], y_train[valid_index]))\n",
        "  print(f'mean validation accuracy: {np.array(scores).mean():.4f} for min_df={min_df} where voc size={X_train.shape[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv6b8YvvXWKN"
      },
      "source": [
        "for min_df in [2, 4, 6, 8]:\n",
        "\n",
        "  vectorizer = TfidfVectorizer(stop_words='english', min_df=min_df, smooth_idf=False, sublinear_tf=True)\n",
        "  X_train = vectorizer.fit_transform(corpus_train)\n",
        "\n",
        "  scores = list()\n",
        "  for train_index, valid_index in skf.split(X_train, y_train):\n",
        "    clf = LogisticRegression(C=1.0, solver='liblinear', random_state=123)\n",
        "    clf.fit(X_train[train_index], y_train[train_index])\n",
        "    scores.append(clf.score(X_train[valid_index], y_train[valid_index]))\n",
        "  print(f'mean validation accuracy: {np.array(scores).mean():.4f} for min_df={min_df} where voc size={X_train.shape[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdJR7om5U_Iq"
      },
      "source": [
        "* 次にmax_dfも変えてみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENow9efuU5cq"
      },
      "source": [
        "for max_df in [1.0, 0.5, 0.4, 0.3, 0.2, 0.1]:\n",
        "\n",
        "  vectorizer = TfidfVectorizer(stop_words='english', min_df=6, max_df=max_df, smooth_idf=False, sublinear_tf=True)\n",
        "  X_train = vectorizer.fit_transform(corpus_train)\n",
        "\n",
        "  scores = list()\n",
        "  for train_index, valid_index in skf.split(X_train, y_train):\n",
        "    clf = LogisticRegression(C=1.0, solver='liblinear', random_state=123)\n",
        "    clf.fit(X_train[train_index], y_train[train_index])\n",
        "    scores.append(clf.score(X_train[valid_index], y_train[valid_index]))\n",
        "  print(f'mean validation accuracy: {np.array(scores).mean():.4f} for max_df={max_df} where voc size={X_train.shape[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnWcZHyJXFzR"
      },
      "source": [
        "## 5) チューニングされたハイパーパラメータでロジスティック回帰の最終的な評価をおこなう\n",
        "\n",
        "* LogisticRegression() のカッコ内には、自分で見つけた最善のセッティングを書き込む。\n",
        "* 学習は、訓練データ全体（テストデータ以外の全体）を使っている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQE3CxUdfZ7V"
      },
      "source": [
        "# 最も良かった設定を使って、訓練データ全体で再学習\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', min_df=6, smooth_idf=False, sublinear_tf=True)\n",
        "X_train = vectorizer.fit_transform(corpus_train)\n",
        "print('# X_train: 文書数　{}, 語彙数 {}'.format(*X_train.shape))\n",
        "\n",
        "# tf-idfの設定を変えたので、語彙も取得し直しておく。\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "clf = LogisticRegression(C=1.0, solver='liblinear', random_state=123)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# そして、最終的にテストデータで評価\n",
        "X_test = vectorizer.transform(corpus_test)\n",
        "print(f'test accuracy: {clf.score(X_test, y_test):.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lvNAU-0vmbH"
      },
      "source": [
        "### 参考: その他の評価尺度"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSO6P2lIvmFF"
      },
      "source": [
        "y_test_pred = clf.predict(X_test)\n",
        "y_test_pred_proba = clf.predict_proba(X_test)\n",
        "\n",
        "# Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('accuracy: {:.4f}'.format(accuracy_score(y_test, y_test_pred)))\n",
        "\n",
        "# Area Under the Receiver Operating Characteristic Curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "print('ROC AUC: {:.4f}'.format(roc_auc_score(y_test, y_test_pred_proba[:,1])))\n",
        "\n",
        "# 様々な評価尺度をまとめてレポート\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "# confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred, labels=clf.classes_)\n",
        "print(cm)\n",
        "\n",
        "# confusion matrixの可視化版\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
        "disp.plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdLcpwSwf3s3"
      },
      "source": [
        "## 6) SVMによる２値分類と評価\n",
        "\n",
        "* SVMについては、私からは実行例を示しません。各自、試行錯誤してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RoCT-f-f3Ow"
      },
      "source": [
        "# これは単純な実行例にすぎません。ハイパーパラメータのチューニングもしてください。\n",
        "scores = list()\n",
        "for train_index, valid_index in skf.split(X_train, y_train):\n",
        "  svm = LinearSVC(random_state=123)\n",
        "  svm.fit(X_train[train_index], y_train[train_index])\n",
        "  scores.append(svm.score(X_train[valid_index], y_train[valid_index]))\n",
        "\n",
        "print(f'mean validation accuracy: {np.array(scores).mean():.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7z1vXs4HfEV"
      },
      "source": [
        "## 7) チューニングされたハイパーパラメータを使って、SVMの最終的な評価をおこなう"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UCt5frCLBly"
      },
      "source": [
        "* 試行錯誤をおこなってから、次のセルを実行してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guAY82oOHevE"
      },
      "source": [
        "# 最も良かった設定を使って、訓練データ全体で再学習\n",
        "\n",
        "svm = LinearSVC(random_state=123)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "X_test = vectorizer.transform(corpus_test)\n",
        "print(f'test accuracy: {svm.score(X_test, y_test):.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CtVT3KEXM7f"
      },
      "source": [
        "## 8) ロジスティック回帰による分類に効いている単語を調べる\n",
        "\n",
        "* 訓練データが最も数が多いので、訓練データの分類に最も効いている単語100語を調べる。\n",
        "\n",
        "* 下に示すのは、あくまで一つの方法にすぎない。他にどんな方法があるか調べて使ってみよう。\n",
        "\n",
        " * 下の手法の欠点は、男性俳優の文書に特徴的な単語と、女性俳優の文書に特徴的な単語とを、区別できない点である。\n",
        "\n",
        " * ヒント： 「svm important features」 あたりでググってみる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctV2xILqH97W"
      },
      "source": [
        "### recursive feature eliminationという特徴量選択の手法\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7An3q7JUK8G"
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# 最も良かった設定を使って、分類器のインスタンスを準備\n",
        "clf = LogisticRegression(C=1.0, solver='liblinear', random_state=123)\n",
        "\n",
        "# RFEで重要な属性上位100個を抽出\n",
        "rfe = RFE(estimator=clf, n_features_to_select=100, step=100)\n",
        "rfe.fit(X_train, y_train)\n",
        "ranking = rfe.ranking_\n",
        "print(vocab[ranking == 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGd-_fRQqQXO"
      },
      "source": [
        "### ロジスティック回帰の係数をそのまま可視化する\n",
        "\n",
        "* 正の係数と負の係数で、それぞれ絶対値が大きいもの30個を可視化する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rIu7hTNmlp7"
      },
      "source": [
        "clf = LogisticRegression(C=1.0, solver='liblinear', random_state=123)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "n_features = 30\n",
        "positions = np.arange(n_features)[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g-CNIfBIya6"
      },
      "source": [
        "* femaleクラスのほうに効いている単語\n",
        " * 必ずしも女性だけに関係するわけではない単語が含まれているか？\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHLsDApX3fng"
      },
      "source": [
        "indices = np.argsort(- clf.coef_[0])[:n_features]\n",
        "widths = clf.coef_[0][indices]\n",
        "yticks = vocab[indices]\n",
        "\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.barh(positions, widths, align='center') \n",
        "plt.yticks(positions, yticks) \n",
        "plt.xlabel('coefficients')\n",
        "plt.title(f'Coefficients of {n_features} Features Using Logistic Regression');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKLvtUPZJAjf"
      },
      "source": [
        "* maleクラスのほうに効いている単語\n",
        " * 必ずしも男性だけに関係するわけではない単語が含まれているか？\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K_Zv4UbkEaM"
      },
      "source": [
        "indices = np.argsort(clf.coef_[0])[:n_features]\n",
        "widths = clf.coef_[0][indices]\n",
        "yticks = vocab[indices]\n",
        "\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.barh(positions, widths, align='center') \n",
        "plt.yticks(positions, yticks) \n",
        "plt.xlabel('coefficients')\n",
        "plt.title(f'Coefficients of {n_features} Features Using Logistic Regression');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzx5_vC5JnA_"
      },
      "source": [
        "## 9) SVMによる分類に効いている単語を調べる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LSWP9vldWJu"
      },
      "source": [
        "* SVMについては実行例を示しません。上を参考に各自試してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poCHDuUEhyXs"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}